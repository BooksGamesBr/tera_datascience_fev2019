{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "![](imgs/nlp_intro.png)\n",
    "\n",
    "***\n",
    "###### Fonte:https://medium.com/greyatom/introduction-to-natural-language-processing-78baac3c602b\n",
    "\n",
    "\n",
    "\n",
    "## O que vem na cabeça de vocês quando falamos de Processamento de Linguagem Natural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição\n",
    "\n",
    "O termo NLP (Natural Laguage Processing, ou Processamento de Linguagem Natural em tradução livre) é um campo da computação relativamente novo e que tem como principal foco fazer com que as máquinas entendam e até possam se comunicar em linguagem humana. É uma área de pesquisa/atuação extremamente ampla, podendo se dividir em ramos com atuações muito diferentes. Alguns dos principais exemplos de atuação são:\n",
    "\n",
    "- Information Retrieval: Com base em uma query do usuário, retornar o produto/documento que atenda suas expectativas (**Google**)\n",
    "- Q&A: Com base em uma pergunta, encontrar a resposta que mais atenda ela (**Watson/Jeopardy**)\n",
    "- Machine Translation: entrar uma linguagem em um idioma e traduzí-la (**Google Translate**)\n",
    "- Information Extraction: \n",
    "![](imgs/avril_height.png)\n",
    "- Sentiment Analysis: é o que veremos hoje !\n",
    "\n",
    "\n",
    "<img src=\"imgs/yeah_gif.gif\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando\n",
    "\n",
    "Suponha que você seja um Cientista de Dados e trabalha em um app de um restaurante. Você conseguiu coletar os reviews de alguns dos seus clientes e pretende pensar como que você agrega valor com eles. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Me and my girls will definitely go back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>What more can you ask for?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What more could you want?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>If you want good authentic Thai this place is not the place to go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>The regular menu here is slightly above average that is not worth the snotty attitude that you receive.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        text\n",
       "342                                                                 Me and my girls will definitely go back.\n",
       "387                                                                               What more can you ask for?\n",
       "31                                                                                 What more could you want?\n",
       "346                                       If you want good authentic Thai this place is not the place to go.\n",
       "362  The regular menu here is slightly above average that is not worth the snotty attitude that you receive."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 999\n",
    "pd.read_csv('data/raw/raw_reviews.csv').sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça um sample dos dados e explore um pouco eles. Entenda que tipo de informação você tem e o que você consegue fazer com ela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO\n",
    "\n",
    "#TENTE ENTENDER UM POUCO SEUS DADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que os reviews querem dizer?\n",
    "\n",
    "**Responda aqui**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "###### **Na verdade, esse conjunto de dados é composto de reviews de restaurantes e foi modificado a partir do dataset usado no workshop SemEval (International Workshop on Semantic Evaluation) na [edição de 2016](http://alt.qcri.org/semeval2016/task5/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tá mas e daí?\n",
    "\n",
    "Se vocês tentarem interpretar o conteúdo, vocês verão que os reviews retratam **opiniões**. Extrapolando um pouco, da pra ver que essas opiniões estão bem **polarizadas** ou em algo *positivo* ou em algo *negativo*. Ora, vocês reconhecem esse tipo de trabalho? E se anotássemos as labels em categorias (positiva ou negativa) e construíssimos um *classificador* para esses reviews?\n",
    "\n",
    "Ou seja, nosso modelo irá receber um **texto** como *feature* e irá retornar uma **classe**: positiva ou negativa. Ou seja, nosso modelo irá **analisar o sentimento** expresso por um certo texto !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is nearly impossible to get a table, so if you ever have the chance to go here for dinner, DO NOT pass it up.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I won't go back unless someone else is footing the bill.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are so many better places to visit!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This place is a must visit!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but the service was a bit slow.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               text  \\\n",
       "0  It is nearly impossible to get a table, so if you ever have the chance to go here for dinner, DO NOT pass it up.   \n",
       "1                                                          I won't go back unless someone else is footing the bill.   \n",
       "2                                                                         There are so many better places to visit!   \n",
       "3                                                                                       This place is a must visit!   \n",
       "4                                                                                   but the service was a bit slow.   \n",
       "\n",
       "   polarity  \n",
       "0  positive  \n",
       "1  negative  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/processed/train.csv')\n",
    "test_df = pd.read_csv('data/processed/test.csv')\n",
    "\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/wat-wat-wat.jpg\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Calma que já vamos entender tudo !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As ferramentas\n",
    "\n",
    "A principal biblioteca existente hoje para trabalhos de NLP é o Spacy. O datacamp lançou recentemente um [curso](https://campus.datacamp.com/courses/advanced-nlp-with-spacy) bem legal sobre ele que vale a pena dar uma conferida já que hoje, focaremos no \"básico\".\n",
    "\n",
    "Outra lib que vale citar é o NLTK, principal biblioteca de NLP até alguns meses atrás.\n",
    "\n",
    "Com o advento de deep learning, outras inciativas também ficaram famosas, como o [AllenNLP](https://allennlp.org/) e o [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/), que são capazes de atingir o estado da arte de muitas aplicações !\n",
    "\n",
    "### O Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando o modelo de inglês\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No centro do Spacy tem o conceito de objeto que contém todo o pipeline de processamento, além de outras regras específicas de uma certa língua. Essa variável é comummente chamada de *nlp*. Por exemplo, para criar um objeto *nlp* da língua inglesa, basta fazer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando você processa um texto com o objecto *nlp*, o *Spacy* cria um *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"It's also attached to Angel's Share, which is a cool, more romantic bar...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *doc* permite você acessar os dados dos seus textos de maneira estruturada. A forma de iteração é de uma sequência de pytho, então:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "'s\n",
      "also\n",
      "attached\n",
      "to\n",
      "Angel\n",
      "'s\n",
      "Share\n",
      ",\n",
      "which\n",
      "is\n",
      "a\n",
      "cool\n",
      ",\n",
      "more\n",
      "romantic\n",
      "bar\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/spacy_01.png)\n",
    "\n",
    "\n",
    "Note que no Spacy, o objeto *Token* é um objeto que possui vários atributos. Alguns importantes/legais:\n",
    "\n",
    "![](imgs/spacy_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Um pouco de prática ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy.lang.____'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5104dcf15592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the Portuguese language class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m____\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m____\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create the nlp object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy.lang.____'"
     ]
    }
   ],
   "source": [
    "# Import the Portuguese language class\n",
    "from spacy.lang.____ import ____\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = ____\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Isso é uma sentença\")\n",
    "\n",
    "# Print the document text\n",
    "print(____.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me\n"
     ]
    }
   ],
   "source": [
    "# Import the Spanish language class and create the nlp object\n",
    "from spacy._____.__ import ____\n",
    "nlp = ______()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"Me gustan los canguros de los árboles y los narvales\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if ____.____:\n",
    "        # Get the next token in the document. The index of the next token in the doc is token.i + 1.\n",
    "        next_token = ____[____]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.____ == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um pouco de estatística\n",
    "\n",
    "É possível carregar alguns modelos pré treinados no Spacy para nos atender em alguns pontos específicos. O caso mais popular é caso queiramos analisar se determinada palavra é um verbo, ou é um nome próprio (po ex: Apple companhia ou apple, a fruta).\n",
    "\n",
    "Esses modelos estatísticos permitem ao spacy prever atriutos linguísticos, principalmente:\n",
    "\n",
    "- Part-of-Speech tags (classificação gramátical)\n",
    "- Nomeação de Entidades (Ex: a Apple companhia, 'Play Photograph')\n",
    "- Words relationship (Dependecy parser)\n",
    "\n",
    "Tais modelos específicos estão divididos em pacotes e precisam ser baixados. O 'en_core_web_sm' é um pacote com vários modelos treinados em inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Atributos de objetos no spacy, quando sem ```_``` representam o índice deles, se não, são o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photograph ORG\n",
      "Nicklelback PERSON\n",
      "yesterday DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I heard Photograph from Nicklelback yesterday !\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note que Photograph está errado. Talvez poderíamos treinar o modelo segundo os nossos dados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I heard photograph from Nicklelback!\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I took the photograph!\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "Não temos nenhuma entidade nos dois ultimos casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercicio\n",
    "\n",
    "Imprima as entidades da frase abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "# Iterate over the entities\n",
    "for ____ in ____.____:\n",
    "    # print the entity text and label\n",
    "    print(____.____, ____.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Para muitas tarefas de NLP, é bom prestar atenção nas chamadas stopwords, que são as palavras mais comuns que aparecem no texto.\n",
    "\n",
    "Como assim? Vamos tokenizar (de maneira simplista, mas note que estamos usando expressões regulares*  e imprimir as palavras mais comuns da coluna text de nosso dataset de treino.\n",
    "\n",
    "Poderíamos usar a ideia de Matcher (ensinada no [curso](https://campus.datacamp.com/courses/advanced-nlp-with-spacy/finding-words-phrases-names-and-concepts?ex=10) do Datacamp), exclusiva do spacy, mas eu acredito que regex são uteis para varias situações !\n",
    "\n",
    "***\n",
    "###### *Para refrescar a memória, você pode usar o [regex golf](https://alf.nu/RegexGolf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 309),\n",
       " ('the', 177),\n",
       " ('a', 103),\n",
       " ('to', 97),\n",
       " ('and', 92),\n",
       " ('i', 90),\n",
       " ('is', 79),\n",
       " ('it', 75),\n",
       " ('for', 56),\n",
       " ('you', 55)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splita as frases por palavras (espaco incluido) e soma elas\n",
    "Counter(sum(train_df['text'].str.lower().str.split(r'[\\W\\s]+').tolist(), [])).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "O que essa regex está fazendo é basicamente convertendo todo o texto para minúsculo e depois 'splitando' em tokens sempre que o texto encontra um caracter em brando (\\s) **ou** sempre que encontra um caracter que **não** é alfanumérico ([^a-zA-Z0-9]), representado pelo \\W\n",
    "\n",
    "Depois, ela pega as 10 ocorrências mais comumns.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É meio intuitivo o que queremos dizer com *stopwords*, correto?\n",
    "\n",
    "No spacy, podemos encontrar as stopwords do nosso modelo assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-800f7b50ae79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#nlp = spacy.load('en_core_web_sm') Ja fizemos isso, so um reminder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0men_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0men_stopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m155\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m161\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load('en_core_web_sm') Ja fizemos isso, so um reminder\n",
    "\n",
    "en_stopwords = sorted([token.text for token in nlp.vocab if token.is_stop])\n",
    "en_stopwords[155:161]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que estamos avaliando a polaridade. Então na hora de analisarmos o texto, seria  horrível perder certas stopwords como palavras de negação, afinal, \"Eu não gosto disso\" é muito diferente de \"Eu gosto disso\"!\n",
    "\n",
    "### Exercicio\n",
    "\n",
    "Explore um pouco a lista e analise as que você acha que podem ser excluidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_excl = ['cannot', 'go', 'off']\n",
    "\n",
    "for w in list_excl:\n",
    "    nlp.vocab[w].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, verificamos quais que removemos da lista de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5de89594d78f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m' -- '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_stopwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'en_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "' -- '.join([w for w in en_stopwords if not nlp.vocab[w].is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização\n",
    "\n",
    "Uma vez que identificamos as palavras que queremos remover por serem muito comuns, agora precisamos **normalizar** o nosso texto. Em outras palavras, precisamos transformar palavras escritas de maneira diferente **no mesmo tipo de escrita**. Por exemplo: *do not*, *dont* e *don't*, apesar de terem sido escritas de maneira diferente **querem dizer a mesma coisa**. Não seria legal se transformássemos elas exatamente na mesma coisa?\n",
    "\n",
    "O processo tradicional de normalizar texto envolve algumas etapas:\n",
    "\n",
    "- expandir as contrações (e.g. don't -> do not);\n",
    "\n",
    "- remover/tratar entidades; (afinal, elas podem gerar ambiguidade... pense na empresa do ipod)\n",
    "\n",
    "- tudo letra minúscula/maiúscula\n",
    "\n",
    "- remover pontuação\n",
    "\n",
    "- remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ain't\": 'are not',\n",
       " \"aren't\": 'are not',\n",
       " \"can't\": 'cannot',\n",
       " \"can't've\": 'cannot have',\n",
       " \"'cause\": 'because',\n",
       " \"could've\": 'could have',\n",
       " \"couldn't\": 'could not',\n",
       " \"couldn't've\": 'could not have',\n",
       " \"didn't\": 'did not',\n",
       " \"doesn't\": 'does not',\n",
       " \"don't\": 'do not',\n",
       " \"hadn't\": 'had not',\n",
       " \"hadn't've\": 'had not have',\n",
       " \"hasn't\": 'has not',\n",
       " \"haven't\": 'have not',\n",
       " \"he'd\": 'he would',\n",
       " \"he'd've\": 'he would have',\n",
       " \"he'll\": 'he will',\n",
       " \"he'll've\": 'he will have',\n",
       " \"he's\": 'he is',\n",
       " \"how'd\": 'how did',\n",
       " \"how'd'y\": 'how do you',\n",
       " \"how'll\": 'how will',\n",
       " \"how's\": 'how is',\n",
       " \"I'd\": 'I would',\n",
       " \"I'd've\": 'I would have',\n",
       " \"I'll\": 'I will',\n",
       " \"I'll've\": 'I will have',\n",
       " \"I'm\": 'I am',\n",
       " \"I've\": 'I have',\n",
       " \"isn't\": 'is not',\n",
       " \"it'd\": 'it would',\n",
       " \"it'd've\": 'it would have',\n",
       " \"it'll\": 'it will',\n",
       " \"it'll've\": 'it will have',\n",
       " \"it's\": 'it is',\n",
       " \"let's\": 'let us',\n",
       " \"ma'am\": 'madam',\n",
       " \"mayn't\": 'may not',\n",
       " \"might've\": 'might have',\n",
       " \"mightn't\": 'might not',\n",
       " \"mightn't've\": 'might not have',\n",
       " \"must've\": 'must have',\n",
       " \"mustn't\": 'must not',\n",
       " \"mustn't've\": 'must not have',\n",
       " \"needn't\": 'need not',\n",
       " \"needn't've\": 'need not have',\n",
       " \"o'clock\": 'of the clock',\n",
       " \"oughtn't\": 'ought not',\n",
       " \"oughtn't've\": 'ought not have',\n",
       " \"shan't\": 'shall not',\n",
       " \"sha'n't\": 'shall not',\n",
       " \"shan't've\": 'shall not have',\n",
       " \"she'd\": 'she would',\n",
       " \"she'd've\": 'she would have',\n",
       " \"she'll\": 'she will',\n",
       " \"she'll've\": 'she will have',\n",
       " \"she's\": 'she is',\n",
       " \"should've\": 'should have',\n",
       " \"shouldn't\": 'should not',\n",
       " \"shouldn't've\": 'should not have',\n",
       " \"so've\": 'so have',\n",
       " \"so's\": 'so is',\n",
       " \"that'd\": 'that would',\n",
       " \"that'd've\": 'that would have',\n",
       " \"that's\": 'that is',\n",
       " \"there'd\": 'there would',\n",
       " \"there'd've\": 'there would have',\n",
       " \"there's\": 'there is',\n",
       " \"they'd\": 'they would',\n",
       " \"they'd've\": 'they would have',\n",
       " \"they'll\": 'they will',\n",
       " \"they'll've\": 'they will have',\n",
       " \"they're\": 'they are',\n",
       " \"they've\": 'they have',\n",
       " \"to've\": 'to have',\n",
       " \"wasn't\": 'was not',\n",
       " \"we'd\": 'we would',\n",
       " \"we'd've\": 'we would have',\n",
       " \"we'll\": 'we will',\n",
       " \"we'll've\": 'we will have',\n",
       " \"we're\": 'we are',\n",
       " \"we've\": 'we have',\n",
       " \"weren't\": 'were not',\n",
       " \"what'll\": 'what will',\n",
       " \"what'll've\": 'what will have',\n",
       " \"what're\": 'what are',\n",
       " \"what's\": 'what is',\n",
       " \"what've\": 'what have',\n",
       " \"when's\": 'when is',\n",
       " \"when've\": 'when have',\n",
       " \"where'd\": 'where did',\n",
       " \"where's\": 'where is',\n",
       " \"where've\": 'where have',\n",
       " \"who'll\": 'who will',\n",
       " \"who'll've\": 'who will have',\n",
       " \"who's\": 'who is',\n",
       " \"who've\": 'who have',\n",
       " \"why's\": 'why is',\n",
       " \"why've\": 'why have',\n",
       " \"will've\": 'will have',\n",
       " \"won't\": 'will not',\n",
       " \"won't've\": 'will not have',\n",
       " \"would've\": 'would have',\n",
       " \"wouldn't\": 'would not',\n",
       " \"wouldn't've\": 'would not have',\n",
       " \"y'all\": 'you all',\n",
       " \"y'all'd\": 'you all would',\n",
       " \"y'all'd've\": 'you all would have',\n",
       " \"y'all're\": 'you all are',\n",
       " \"y'all've\": 'you all have',\n",
       " \"you'd\": 'you would',\n",
       " \"you'd've\": 'you would have',\n",
       " \"you'll\": 'you will',\n",
       " \"you'll've\": 'you shall have',\n",
       " \"you're\": 'you are',\n",
       " \"you've\": 'you have',\n",
       " \"doin'\": 'doing',\n",
       " \"goin'\": 'going',\n",
       " \"nothin'\": 'nothing',\n",
       " \"somethin'\": 'something'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from utils.contractions import CONTRACTIONS_DICT\n",
    "CONTRACTIONS_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, temos uma função que pega uma certa palavra contraída e \"expande\" ela :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    if \"'\" in text:\n",
    "        for contracted, expanded in CONTRACTIONS_DICT.items():\n",
    "            text = re.sub(contracted, expanded, text, flags=re.I)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(expand_contractions(\"It's\").lower() == 'it is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercicio !!\n",
    "\n",
    "Crie uma função ```normalize_text``` que recebe qualquer string como entrada e faz as seguintes coisas, nessa ordem:\n",
    "\n",
    "- converte tudo para lowercase\n",
    "- expande as contracoes\n",
    "- remove entidades\n",
    "- remove pontuacao e stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ents(text):\n",
    "    ##FUNCAO AUXILIAR\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    ##FUNCAO AUXILIAR  \n",
    "    return ' '.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(normalize_text(\"It's also attached to Angel's Share, which is a cool, more romantic bar...\") == 'attached cool romantic bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplique sua função normalize_text à coluna text (dos dois dataframes, train_df e test_df), criando uma coluna nova norm_text, que será usada para treinarmos um modelo de análise de sentimento. Dica: Use o apply do pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de normalização não é muito canônico (único). Você pode (e deve!) rodar as duas células acima algumas vezes para checar os resultados da normalização. Caso não se sinta satisfeito com a normalização feita, volte e edite as stopwords ou sua função de normalização até se sentir satisfeito(a) com os resultados :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O Modelo\n",
    "\n",
    "<img src=\"imgs/relief.gif\" align=\"center\"/>\n",
    "\n",
    "Depois de todo esse trabalho de **pré processamento**, podemos finalmente nos dedicar ao modelos. Mas **ué** como que fazemos esses textos features?\n",
    "\n",
    "**Lembrem-se sempre**. Computadores entendem apenas números. O que temos que fazer, então, é transformar os nossos dados em números !\n",
    "\n",
    "\n",
    "Mas como??? E se usássemos um dicionário?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_for_bow = [\n",
    "    'camisa preta botao botao botao',\n",
    "    'botao feito linha preta',\n",
    "    'considera-se caro preco botao camisa botao',\n",
    "    'linha costurar botão mesma camisa',\n",
    "    'costurar linha camisa mesma botao'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=5, strip_accents='unicode', binary=True)\n",
    "\n",
    "bow_matrix = cv.fit_transform(examples_for_bow)\n",
    "bow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 1, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(botao, 0)</th>\n",
       "      <th>(camisa, 1)</th>\n",
       "      <th>(costurar, 2)</th>\n",
       "      <th>(linha, 3)</th>\n",
       "      <th>(mesma, 4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (botao, 0)  (camisa, 1)  (costurar, 2)  (linha, 3)  (mesma, 4)\n",
       "0           1            1              0           0           0\n",
       "1           1            0              0           1           0\n",
       "2           1            1              0           0           0\n",
       "3           1            1              1           1           1\n",
       "4           1            1              1           1           1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow_matrix.todense(), columns=sorted(cv.vocabulary_.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse tipo de codificação não leva em conta as frequências das palavras, mas apenas a presença delas no texto (sem contar a ordem). Esse tipo de modelo é conhecido como *sacola de palavras* ou **Bag of Words**\n",
    "\n",
    "## Exercicio\n",
    "\n",
    "Monte um pipeline (criando um objeto do tipo [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)), que inclua a transformação do texto em features numéricas (CountVectorizer) e o classificador LogisticRegression. Treine o modelo usando esse pipeline com os dados de input de train_df. Encontre a probabilidade do texto de pertencer a cada sentimento.\n",
    "\n",
    "Dica: Lembre-se de transformar o target (coluna polarity) em 0s e 1s - 0 para negative e 1 para positive.\n",
    "\n",
    "###### Dica 2: https://medium.com/@baemaek/text-mining-preprocess-and-naive-bayes-classifier-da0000f633b2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando o resultado\n",
    "\n",
    "Nos dados de teste, faça a predição da coluna norm_text e compare o resultado com o vetor target (coluna polarity em 0s e 1s).\n",
    "\n",
    "Dica: Imprima o [classification_report](https://scikit-learn.org/0.19/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) e a [matriz de confusão](https://scikit-learn.org/0.19/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix).\n",
    "\n",
    "Mais importante que o nosso modelo, é entendermos qual métrica queremos melhorar. O que é mais importante aqui? Discutam em grupos !\n",
    "\n",
    "Vamos apresentar isso para os stakeholders !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.confusion_matrix import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mas dá pra melhorar?\n",
    "\n",
    "Note que se usássemos binary=False, estaríamos contando as ocorrências das palavras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9d672b7d0577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbow_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_for_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbow_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=5, strip_accents='unicode', binary=False)\n",
    "\n",
    "bow_matrix = cv.fit_transform(examples_for_bow)\n",
    "bow_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mais feature Engineering\n",
    "\n",
    "Compare os casos BoW e Term Frequency. Dá pra melhorar?\n",
    "\n",
    "Sim ! Dê uma olhada no [Tf-Idf](https://pt.wikipedia.org/wiki/Tf%E2%80%93idf), que é uma métrica mais \"contextual\" e implemente sua versão. Avalie os resultados.\n",
    "\n",
    "[Dica](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curiosidade:\n",
    "\n",
    "Teste os seus dados com uma Random Forest. Eu não testei, mas aposto que é pior. Por que?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Da pra fazer ainda mais?\n",
    "\n",
    "- [Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)\n",
    "\n",
    "Depois da aula de Deep Learning !\n",
    "\n",
    "- [W2V](https://www.distilled.net/word2vec-examples/) ou [aqui](https://skymind.ai/wiki/word2vec)\n",
    "- [BERT](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "###### Grande parte dessa aula foi inspirada na aula da Cintia do ultimo bootcamp, [lá ](https://github.com/somostera/tera-datascience-out2018/blob/master/26-nlp) tem alguns detalhes a mais de comparação do Spacy com o NLTK e algumas outras coisas que eu resolvi abordar diferente aqui :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
